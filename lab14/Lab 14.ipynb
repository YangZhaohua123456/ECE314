{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal component analysis (PCA) and clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this lab is to illustrate how principal component analysis can be used to reduce the number of dimensions of a data set while preserving the most significant variations among the data points.  We'll see visually that if there are clusters planted in the data, then they will emerge.  Although we won't get into it in this lab, basic clustering algorithms such as the K-means algorithm, could be applied to the reduced dimensional data to identify the clusters instead of our eyeballs method.  We'll also see the importance of standardizing the data in case the measurements of different data coordinates are on drastically different scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is our typical list of imports. You may notice that this is a bit different or more complex from some of the previous labs. This code is just allowing us to do some fancier things with some of our graphs. Feel free to look up some of the modules if you are interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats as st\n",
    "from scipy.stats import multivariate_normal\n",
    "import csv\n",
    "from mpl_toolkits.mplot3d import Axes3D   #Toolkits are collections of application-specific functions that extend matplotlib\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "\n",
    "class Arrow3D(FancyArrowPatch):  # Arrow3D([a,d],[b,e],[c,f]) draws arrow (a,b,c)--> (d,e,f)\n",
    "    def __init__(self, xs, ys, zs, *args, **kwargs):\n",
    "        FancyArrowPatch.__init__(self, (0,0), (0,0), *args, **kwargs)\n",
    "        self._verts3d = xs, ys, zs\n",
    "\n",
    "    def draw(self, renderer):\n",
    "        xs3d, ys3d, zs3d = self._verts3d\n",
    "        xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, renderer.M)\n",
    "        self.set_positions((xs[0],ys[0]),(xs[1],ys[1]))\n",
    "        FancyArrowPatch.draw(self, renderer)\n",
    "print ('Modules Imported!')\n",
    "\n",
    "#  From .csv file, read headers, then load numbers into array x\n",
    "#  Warning: unfortunately there are many types of .csv files\n",
    "#  This code assumes the .csv file has comma separate heading in the first row\n",
    "#  and comma separated numbers in the remaining rows.\n",
    "def data_load(str):\n",
    "    csv_path = str\n",
    "    with open(csv_path,'rt') as csvfile:  #After code under \"with open as\" is completed, csvfile is closed\n",
    "        reader=csv.reader(csvfile)\n",
    "        headings=next(reader)\n",
    "        print (\"Reading csv file with headers:\\n  \",\"\\n   \".join(headings),\"\\n\")\n",
    "        x=[]\n",
    "        for row in reader:\n",
    "            x.append(row)    \n",
    "    return(np.array(x,dtype=float).T)   # returns data with one column for each multidimensional sample\n",
    "\n",
    "print (\"Function loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we consider the principal components analysis (PCA) of the three dimensional data formed by (quiz sum, exam 1, exam 2) scores in the ECE 313 data.  At this point we will no longer be using the $Y$ values, becasue we want to try visualizing the data, and we're limited to visualizing three dimensions at a time.   Below you will work with 15 dimensional data, but use dimensionality reduction to visualize the data in two or three dimenions.  By starting out thinking about three dimensional data, we can see it all!   If the three variables, in this case quiz sum, exam 1, and exam 2, were uncorrelated, the structure of the geometry of the data would be pretty simple.   It is not difficult to guess what a scatterplot of the data would look like, if we know the means and variances of each of the scores.   The idea of PCA is that any probability distribution (such as the empirical distribution we are using) has uncorrelated coordinates under a change in coordinate system that preserves distances.   The key is the eigen decomposition of the covariance matrix, as illustrated in the next two boxes of code.  The eigen vectors produced (they are the columns of the matrix eig\\_vec) are orthogonal length one vectors that represent the new coordinate system, and the eigenvalues represent the variances of the data along the directions of each of the eigen vectors.  The code below prints the square roots of the eigen values, which gives the standard deviations in the different new coordinate directions.\n",
    "\n",
    "The last assertion of the code checks that the covariance matrix has the following representation in terms of the eigenvalues $\\lambda_i$ and eigenvectors $v_i$:\n",
    "$$  \\Sigma_{XX} = \\sum_{i=1}^3  \\lambda_i v_iv_i^T   .$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Load the 313scores and compute covariance matrix and mean\n",
    "###  for the first three scores (quiz sum, exam 1, exam 2)\n",
    "\n",
    "x=data_load(\"313scores.csv\")   # One column per student, x.shape[1] is number of students\n",
    "dim=3  # Number of scores per student we'll work with\n",
    "mean_x=np.mean(x,axis=1).reshape(-1,1)   # reshape makes mean_x a 2-d array of width one\n",
    "                                         # so when we subtract mean_x from the matrix x\n",
    "                                         # the subtraction is done for each column of x\n",
    "                                         # The -1 is a wildcard value for height\n",
    "covariance_matrix = np.dot(x-mean_x,(x-mean_x).T)/(x.shape[1])\n",
    "EX=mean_x[0:dim]\n",
    "CovXX=covariance_matrix[0:dim,0:dim]\n",
    "\n",
    "\n",
    "# eigendecomposition of covariance matrix CovXX\n",
    "# The eigen vectors are placed in the columns of the matrix eig_vec\n",
    "# Note that CovXX is computed above\n",
    "\n",
    "eig_val, eig_vec = np.linalg.eig(CovXX)\n",
    "print (\"square roots of eigenvalues:\", np.sqrt(eig_val),\"\\neigen vectors (as columns):\", '\\n', eig_vec)\n",
    "        \n",
    "assert (CovXX == np.dot(eig_vec,np.dot(np.diagflat(eig_val),eig_vec.T))).all, \"eigen decomposition\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we think of a scatterplot of the data as forming a blob in three dimensional space, then the mean vector is located at the center of mass of the blob.   If the blob is generated by a large number of identically Gaussian random variables, its level sets would be concentric elipses.  The principal axes of the elipses are given by the three eigenvectors, and the relative thickness of the elipses in the three dimensions are given by the square roots of the eigenvectors.   Those values give the standard deviation of the data in the three principal directions.  This geometric picture is illustrated by the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principal component analysis (PCA) representation of 3d data\n",
    "#\n",
    "# Scatter plot of data, overlaid with the three eigen vectors, scaled by\n",
    "# square root of eigenvalues, and all translated to the sample mean.\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111, projection='3d')  #ax is a 3-d aware axes object \n",
    "\n",
    "MX=EX.reshape(-1)  # MX is plain np.array version of 3x1 np.array EX\n",
    "ax.plot(x[0,:], x[1,:],\\\n",
    "    x[2,:], 'o', markersize=8, color='blue', alpha=0.5)\n",
    "ax.plot([MX[0]],[MX[1]],[MX[2]], 'o', \\\n",
    "    markersize=10, color='orange', alpha=1.0)\n",
    "print (\"Mean vector:\",MX,\"\\nSquare roots of eigenvalues:\", np.sqrt(eig_val))\n",
    "\n",
    "for i in range(3):   #Iterate through (eigenvalue, eigenvector) pairs\n",
    "    a = Arrow3D([MX[0], MX[0]+np.sqrt(eig_val[i])*eig_vec[0,i]], \\\n",
    "                [MX[1], MX[1]+np.sqrt(eig_val[i])*eig_vec[1,i]], \\\n",
    "                [MX[2], MX[2]+np.sqrt(eig_val[i])*eig_vec[2,i]], \\\n",
    "                     mutation_scale=20, lw=3, arrowstyle=\"-|>\", color=\"orange\")\n",
    "    ax.add_artist(a)\n",
    "ax.set_xlabel('quiz sum')\n",
    "ax.set_ylabel('exam 1')\n",
    "ax.set_zlabel('exam 2')\n",
    "\n",
    "plt.title('Scatter plot of data and PCA representation')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<SPAN style=\"BACKGROUND-COLOR: #C0C0C0\">Problem 1:</SPAN>** Use the code given above to help you with the following: \n",
    "<ol><li>Generate $N=1000$ samples of three dimensional Gaussian random vectors with mean zero and covariance matrix $\\Sigma =\\left(\\begin{array}{ccc} 1& 0 & 0 \\\\0 & 4 & 0 \\\\ 0 & 0 & 9  \\end{array}\\right).$ (Hint: You could use multivariate_normal.rvs, as used in previous lab and used below.)\n",
    "<li> Calculate the covariance matrix, $\\widehat{\\Sigma},$ for the emirical distribution of the data you created. (Some code in Lab 13 might be useful here.)\n",
    "<li> Calculate the Frobenious norm of the estimation error matrix, $\\Sigma - \\widehat{\\Sigma}.$  (Hint: The Frobenius norm of a matrix is the square root of the sum of the squares of the elements, and is computed by np.linalg.norm()).\n",
    "<li> Calculate the square roots of the eigenvales and the eigenvectors for both $\\Sigma$ and $\\widehat{\\Sigma}$\n",
    "and compare.  (They should be close, but possibly in different order, and the eigenvectors could be multiplied through by -1.)\n",
    "<li>  Calculate and display a 3D scatter plot of the data with the PCA representation of the data overlaid.  Comment on how accurately the eigenvectors and eigenvalues are captured by the data for $N=100$ and $N=1000.$  (Hint: Decrease the transparency value alpha when more points are plotted so the arrows can still be seen.) \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__: (Your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<SPAN style=\"BACKGROUND-COLOR: #C0C0C0\">End of Problem 1</SPAN>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will use the PCA method to examine some data where each sample has more than three dimensions.  We'll try 15 dimensions.   It is perhaps impossible for humans to visualize fifteen dimensional space, so for the purposes of visualization (and other tasks, such as clustering) we would like to reduce the dimensionality of the data, while trying to preserve the structure of the data as much as possible.  An obvious way to reduce the dimensionality is to use only three of the coordinates of each data sample. If, for example, we use three out of fifteen coordinates for each data sample, we are essentially ignoring 80% of the data.  We may be able to improve on that by trying different sets of three coordinates to look at.    The principal components representation (PCA) of the data offers a better approach.   The eigenvectors produced by python are normalized to be unit length vectors, and they are orthogonal to each other.   That means they are suitable for a change of coordinates.   Indeed, if $eigvec$ is the matrix such that the columns are the eigenvectors, and if $y=eigvec^T*x$ (using ordinary matrix times vector multiplication),  then $y$ represents the same point as $x,$  but relative to the basis formed by the eigen vectors.\n",
    "\n",
    "The next problem explores this question for some randomly generated data in which each data vector has one of four unobserved random types.  The next cell generates the data.  For the first three scores of ECE 313 data, and for the Gaussian data you generated above, the scatter plots should form an elliptical blob of data.   However, for the data generated next, the data vectors of different types have different means.  All the means lie in a two or three dimensional space.   That causes the data to be more spread out in the directions that the mean vectors are in.   Thus, by doing a PCA analysis, the directions with the largest and second largest eigenvalues, will allow us to see the separated clusters (if the distance between cluster centers is large enough compared to the radii of the clusters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data15()  DATA GENERATOR FUNCTION   Start with standard 15 dimensional Gaussian samples.\n",
    "## Add a mean vector to each sample, selected at random from among the four columns\n",
    "## of mu using probability vector p.  For the given example, mu has rank two\n",
    "## and a 2-D plot is able to distinguish the four clusters\n",
    "## Signal factor scales the distance between cluster centers\n",
    "def data15(signal_factor, num_samples, version):   # version should be 0,1,or 2.\n",
    "    print (\"signal_factor=\",signal_factor, \"num_samples=\",num_samples,\"  version=\",version)\n",
    "    dim=15  # Dimension of the random vectors\n",
    "    planted_cov=np.identity(dim)\n",
    "    x15 = multivariate_normal.rvs(np.zeros(dim),planted_cov,num_samples).T\n",
    "    mu=np.array([[0, 0, 0, 0, 0,  0,  0,  0,  0,  0, 0,0,0,0,0],\n",
    "                 [0, 0, 0, 0, 0,  1,  1,  1,  1,  1, 0,0,0,0,0],\n",
    "                 [0, 0, 0, 0, 0,  0,  0,  0,  0,  0, 1,1,1,1,1],\n",
    "                 [0, 0, 0, 0, 0,0.5,0.5,0.5,0.5,0.5, 1,1,1,1,1]]).T\n",
    "    # The four column vectors of mu have dimension 15.  The first is zero, and the\n",
    "    # fourth one is a linear combination of the second and third ones.  Thus, the four\n",
    "    # mean vectors span a two dimensional space.   If we can approximately identify\n",
    "    # that space from the data, it would help us identify the clusters.\n",
    "    # However, for version 1 of this data generator, the last column of\n",
    "    # mu is changed so that the columns of mu span a three dimensional space:\n",
    "    if version==1:\n",
    "        mu[:,3]=np.array([0,0,0,1,1,0,0,1,1,1,1,1,0,0,0]).T\n",
    "    # For any version, a random type for each sample selects the mean vector\n",
    "    # Since there are four possible mean vectors, the generated data will tend\n",
    "    # to be in four clusters.   We'll see if we can use PCA to reveal the\n",
    "    # clusters in the data.\n",
    "    p = (.1,.2,.3,.4) #Tuple of probabilities gives rough fractions of samples of each type\n",
    "    c = (0,1,2,3) #Tuple of possible types\n",
    "    sample_type = st.rv_discrete(name='Xcstm',values=(c,p)).rvs(size=num_samples)\n",
    "    for i in range(num_samples):\n",
    "        x15[:,i] += signal_factor*mu[:,sample_type[i]]\n",
    "    if version==2:  # For version 2 we scale different coordinates of the data differently.\n",
    "                    # The scaling could correspond to various units, seconds,kms, meters, etc.\n",
    "        v=np.ones(dim).reshape(dim,1)\n",
    "        v[2:5]=100\n",
    "        v[6]=4.0\n",
    "        v[8]=0.3\n",
    "        x15=x15*v  # For each column of x15 does entry by entry multiplication by v.\n",
    "    return x15\n",
    "#########################\n",
    "#########################\n",
    "\n",
    "print (\"Example data with\",data15(12,3,version=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<SPAN style=\"BACKGROUND-COLOR: #C0C0C0\">Problem 2:</SPAN>**    \n",
    "<ol>\n",
    "<li> Generate a 3-D and a 2-D scatter plot based on the first three or first two coordinates of the data produced using data15(signal\\_factor=12,1000,version=0).  Do you observe any clusters or groupings of the data?\n",
    "<li> Generate a 3-D and a 2-D scatter plot based on the last three or last two coordinates. Are there any clusters now?\n",
    "<li> Transform the data to the new coordinates $y$ by multiplying each data vector by the transpose of the eigen vector matrix for the empirical covariance matrix (i.e., in python, y=np.dot(eig_vec.T,x)). Again, generate a 3-D plot based on the first three coordinates of the data and a 2-D plot based on the first two coordinates of the data. Can you see clusters now? Give a reason as to why this might be the case.\n",
    "<li> Try varying the parameter signal\\_factor from 1 to 20.   Approximately for what values of signal\\_factor is it possible to see all four clusters (using either the 2D or 3D plots, whichever works better) under the PCA transformation?\n",
    "<ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__ (Your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<SPAN style=\"BACKGROUND-COLOR: #C0C0C0\">End of Problem 2</SPAN>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<SPAN style=\"BACKGROUND-COLOR: #C0C0C0\">Problem 3:</SPAN>**\n",
    "Repeat the previous problem, but use data generated by data15(12,1000,1) instead. Briefly compare the results in this question to the results in the previous question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__ (Your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<SPAN style=\"BACKGROUND-COLOR: #C0C0C0\">End of Problem 3</SPAN>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In versions 0 and 1 of data15, the variance of the Gaussian part of the data is the same in all directions.  But in many applications the different coordinates could represent measurements in unrelated units and could have drastically different sample variances.   For example, one dimension might represent height in feet, another weight in grams, another temperature in degrees centigrade, and so forth.    In that case the PCA analysis, which identifies the directions in which the data has the greatest variation, could simply pick out which coordinates of the data are measured in the smallest units.   For example, changing distance measurements from kilometers to centimeters increases the variance by $10^{10}$!   To illustrate this, version 2 of data15 uses different variances for the different coordinates of the data.   In the problem, you will still seek to find the clusters.   First, you should check to see what happens without some pre-scaling.\n",
    "\n",
    "**<SPAN style=\"BACKGROUND-COLOR: #C0C0C0\">Problem 4:</SPAN>**  For this problem, analyze data generated data15 version 2:  data15(12,1000,2).   \n",
    "<ol>\n",
    "<li>  To see that the nonuniform scaling causes problems, try identifying the clusters using the two or three dimensional plots based on the first two or three coordinates in the dimensions provided by the eigen decomposion of the sample covariance matrix.\n",
    "<li>  A possible fix is to first standardize the data by dividing the data in each coordinate by the empirical standard deviation for that coordinate.    Note that the covariance matrix for the standardized data is the matrix of correlation coefficients with ones down the diagonal.   Apply the PCA analysis to the standardized data to see if you can detect the cluster structure for much smaller values of signal_factor. \n",
    "</ol>\n",
    "\n",
    "Congratulations on finshing the last lab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__ (Your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<SPAN style=\"BACKGROUND-COLOR: #C0C0C0\">End of Problem 4</SPAN>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "## Academic Integrity Statement ##\n",
    "\n",
    "By submitting the lab with this statement, you declare you have written up the lab entirely by yourself, including both code and markdown cells. You also agree that you should not share your code with anyone else. Any violation of the academic integrity requirement may cause an academic integrity report to be filed that could go into your student record. See <a href=\"https://provost.illinois.edu/policies/policies/academic-integrity/students-quick-reference-guide-to-academic-integrity/\">Students' Quick Reference Guide to Academic Integrity</a> for more information. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
