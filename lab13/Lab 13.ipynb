{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 13: Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab covers both simple and multiple linear regression.</B>\n",
    "\n",
    "Below is our typical list of imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats as st\n",
    "from scipy.stats import multivariate_normal\n",
    "import csv\n",
    "print ('Modules Imported!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review from Lab 10: Covariance matrices and multivariate Gaussian distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ECE 313 we consider the bivariate Gaussian distribution. Recall that we used the bivariate Gaussian distribution near the end of Lab 10 for modeling a hypothesis testing problem. It is a joint distribution for two random variables, $X_1,X_2$ and is uniquely determined by five parameters, the means of the two random variables, $m_1$ and $m_2$, the variances of the two random variables, and the covariance between the two random variables defined by $\\mbox{Cov}(X_1,X_2)=E[(X_1-m_1)(X_2-m_2)].$ \n",
    "When we think of multidimensional distributions, we often use Cov(,) to describe all kinds of covariances. Note that $\\mbox{Cov}(X_1,X_1)=\\mbox{Var}(X_1)$ and $\\mbox{Cov}(X_1,X_2)=\\mbox{Cov}(X_2,X_1).$    Equivalently,\n",
    "we can think of $\\binom{X_1}{X_2}$ as a random vector, with  a mean vector $\\binom{m_1}{m_2}$ and a covariance matrix\n",
    "$\\Sigma=\\left( \\begin{array}  \\mbox{Cov}(X_1,X_1) & \\mbox{Cov}(X_1,X_2)\\\\ \\mbox{Cov}(X_2,X_1) & \\mbox{Cov}(X_2,X_2) \\end{array}\\right).$ Based on our definitions shown above, it's clear that a covariance matrix is symmetric (i.e. $\\Sigma^T = \\Sigma$).  Joint normal (also known as joint Gaussian) distributions exist in any number of dimensions.   A Gaussian distribution in a given number of dimensions is specified uniquely by a mean vector and a covariance matrix.  The following code copied from Lab 10 generates variates for two normal distributions.  The orange triangles follow a distribution that is rotationally symmetric about the origin.   The blue circles follow a distribution with positive correlation between the two coordinates; the shape of the blue blob of points is elongated along a line of slope one.  Also, the mean vector for the blue points is $\\binom{2.0}{0}$ so the blue blob is offset a bit to the right of the orange blob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=2  # Dimension of the random vectors\n",
    "num_samples=200\n",
    "Sigma0=2.0*np.identity(dim)    # identity matrix\n",
    "Sigma1=np.identity(dim)+ 4.0*np.ones([dim,dim])   # some positive correlation added\n",
    "mu0=np.zeros(dim)\n",
    "mu1=np.zeros(dim)\n",
    "mu1[0]=2.0  # first coordinate has nonzero mean under H1\n",
    "variates0= multivariate_normal.rvs(mu0,Sigma0,num_samples)\n",
    "variates1= multivariate_normal.rvs(mu1,Sigma1,num_samples)\n",
    "plt.scatter(variates0[:,0],variates0[:,1],color='orange',marker='^')\n",
    "plt.scatter(variates1[:,0],variates1[:,1],color='blue')\n",
    "# plt.plot.scatter(variates2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple linear regression, aka $\\widehat{E}[Y|X]~$ based on data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a pair of correlated random variables, X and Y, and we wanted to estimate Y based on the observation of X. In doing this, we want to minimize the mean square error between Y and our estimate of Y. As seen in Section 4.9.2 of the ECE 313 notes, the estimator that achieves this is simply given by $E[Y|X]$. This can be difficult or impractical to calculate so we like to look at a linear estimator. As seen in Section 4.9.3 of the ECE 313 notes, the minimum mean square error linear estimator of a random variable $Y$ based on observation of a random variable $X$ is $\\widehat{E}[Y|X]=L^*(X),$ where $L^*$ is the linear (actually, affine) function defined by $L^*(u)=\\mu_Y + \\frac{\\mbox{Cov}(Y,X)}{\\mbox{Var}(X)}(u-\\mu_X).$   Often we don't have a known joint distribution of $X$ and $Y$ but we have a list of samples of $(X,Y)$ pairs, $((x_i,y_i): 0\\leq i \\leq n-1).$     We can use the same equations for the linear estimator $L$ by using the *empirical* joint distribution of $(X,Y),$  where the empirical joint distribution gives equal probability $1/n$ to each sample pair. Below is code that generates a lot of sample pairs from a joint Gaussian distribution and then finds the linear estimator of the Y coordinate based on the X coordinate. This is known as linear regression of the $Y$ values based on the $X$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "def linear_estimator(x,y):\n",
    "# Given samples produce the optimal linear estimator Lstar(u)\n",
    "# Note that this function returns a function\n",
    "# Both the observations (the x's) and the values to be\n",
    "# estimated (the y's) are assumed to be scaler valued.\n",
    "    num_samples = len(x)\n",
    "    assert len(x)==len(y), \"number of x values unequal to number of y values\"\n",
    "    ave_x = np.average(x)\n",
    "    ave_y = np.average(y)\n",
    "    var_x = np.average(x*x)-ave_x**2\n",
    "    cov_xy = np.average(x*y)-ave_x*ave_y\n",
    "    \n",
    "    a = cov_xy/var_x\n",
    "    b = ave_y-ave_x*a\n",
    "    \n",
    "    def Lstar(u):     # Define the function to be returned\n",
    "        return a*u+b\n",
    "    \n",
    "    return  Lstar;\n",
    "#########################\n",
    "\n",
    "num_samples=100\n",
    "\n",
    "###  Randomly generate synthetic data using a joint Gaussian distribution\n",
    "Sigma=np.array([[4,-2],[-2,2]])  \n",
    "mu=np.array([3,1])\n",
    "data = multivariate_normal.rvs(mu,Sigma,num_samples).T   #shape=(2,num_samples)\n",
    "# Think of the first row of data as the X values, to be observed, and the second\n",
    "# row of data as the Y values, to be estimated.\n",
    "\n",
    "\n",
    "Lstar = linear_estimator(data[0,:],data[1,:])  #  Produces Lstar, a linear function of one variable\n",
    "\n",
    "u=np.linspace(min(data[0,:]),max(data[0,:]),1000)\n",
    "plt.plot(u,Lstar(u))\n",
    "plt.scatter(data[0,:],data[1,:]),\n",
    "plt.title('data with estimator function overlaid')\n",
    "plt.xlabel('X values')\n",
    "plt.ylabel('Y values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<SPAN style=\"BACKGROUND-COLOR: #C0C0C0\">Problem 1:</SPAN>**  We can similarly compute $\\widehat{E}[X|Y]$,  which gives a linear estimate of $X$ as a function of $Y.$  \n",
    "\n",
    "1. Calculate the estimator $\\widehat{E}[X|Y]$.\n",
    "2. Plot it on the same set of axes as $\\widehat{E}[Y|X].$    \n",
    "3. Does it give the same linear relationship between $X$ and $Y$ that $\\widehat{E}[Y|X]$ does?  Explain or prove your answer in general for two random variables $X$ and $Y$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__ (Your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<SPAN style=\"BACKGROUND-COLOR: #C0C0C0\">End of Problem 1</SPAN>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear predictors for ECE 313 final exam scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code loads some scores from a CSV file.  Each row in the file corresponds to four scores for a particular student: quiz sum, Exam 1, Exam 2, and final exam, for ECE 313 at the University of Illinois in Spring 2013. (The quiz sum is the sum for all 12 quizzes with no dropping of three lowest quiz scores.) The function data_load() takes a transpose, so that each column of the numpy array returned by the function corresponds to one student."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  From .csv file, read headers, then load numbers into array x\n",
    "#  Warning: unfortunately there are many types of .csv files\n",
    "#  This code assumes the .csv file has comma separate headings in the first row\n",
    "#  and comma separated numbers in the remaining rows.\n",
    "def data_load(string):\n",
    "    csv_path = string\n",
    "    with open(csv_path,'rt') as csvfile:  #After code under \"with open as\" is completed, csvfile is closed\n",
    "        reader=csv.reader(csvfile)\n",
    "        headings=next(reader)\n",
    "        print (\"Reading csv file with headers:\\n  \",\"\\n   \".join(headings),\"\\n\")\n",
    "        x=[]\n",
    "        for row in reader:\n",
    "            x.append(row)    \n",
    "    return(np.array(x,dtype=float).T)   # returns data with one column for each multidimensional sample\n",
    "\n",
    "print (\"Function loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the ECE 313 scores, and calculate the sample covariance matrix and sample correlation matrix. Recall that the correlation coefficient between two random variables $X$ and $Y$ is given by $\\rho_{X,Y} = \\frac{\\mbox{Cov}(X,Y)}{\\sigma_X \\sigma_Y}.$    The correlation matrix is the matrix of correlation coefficients.  (``Correlation coefficient matrix,\" would be a more accurate name but it's too long to use.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean vector, covariance matrix, and correlation matrix for empirical distribution\n",
    "#\n",
    "x=data_load(\"313scores.csv\")\n",
    "mean_x=np.mean(x,axis=1).reshape(-1,1)   # return mean as 4x1 column vector\n",
    "      # -1 means \"unspecified value,\" the value is inferred to be 4 for the 313scores data\n",
    "covariance_matrix = np.dot(x-mean_x,(x-mean_x).T)/(x.shape[1])\n",
    "print (\"mean=\", '\\n', mean_x, '\\n\\n', \"covariance matrix=\",'\\n',covariance_matrix,'\\n')\n",
    "\n",
    "variances=np.diag(covariance_matrix).reshape(-1,1)  # column vector of variances\n",
    "std_dev=np.sqrt(variances)    # vector of stdev's\n",
    "\n",
    "print (\"variances=  (diagonal entries of the covariance matrix)\", '\\n', \\\n",
    "        variances,'\\n\\n', \"standard deviations=\",'\\n', std_dev, '\\n')\n",
    "\n",
    "correlation_matrix=covariance_matrix/std_dev/std_dev.T\n",
    "      # In numpy, division of a matrix (a_ij) by a row vector (v_j) divide a_ij by v_j\n",
    "      # and division of (a_ij) by a column vector (v_i) divides a_ij by v_i\n",
    "print (\"correlation matrix=  (i.e. matrix of correlation coefficients)\",'\\n', correlation_matrix,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the mean error is for estimating the score on the final from any one of the other three scores. Here we recall from the ECE 313 notes (Section 4.9.3) that to estimate a random variable $Y$ from a random variable $X,$  the minimum mean square error linear estimator is given by\n",
    "$$ \\widehat{E}[Y | X] =L^*(X) = E[Y] +\\frac{ \\mbox{Cov}(Y,X) }{\\mbox{Var}(X) } (X-E[X])$$\n",
    "$$= E[Y] + \\frac{\\sigma_X \\sigma_Y \\rho_{X,Y}}{\\sigma_X^2} \\left(X-E[X]\\right)$$\n",
    "$$= E[Y] + \\sigma_Y \\rho_{X,Y} \\left( \\frac{X-E[X]}{\\sigma_X } \\right)$$\n",
    "and the mean square error is given by:\n",
    "$$E[(Y-\\widehat{E}[Y | X])^2] = \\mbox{Var}(Y) - \\frac{ \\mbox{Cov}(Y,X)^2}{ \\mbox{Cov}(X,X)} = \\sigma_Y^2(1-\\rho_{X,Y}^2),$$   where $ \\rho_{X,Y} $ is the correlation coefficient between $X$ and $Y.$\n",
    "To apply these equations to our data, we can choose any one of the first three scores to represent $X$  and the final exam score to represent $Y.$  So we could estimate the final score from any of the other three scores.  In the code below we don't actually calculate the linear estimators (though it could be easily done, using the function linear\\_estimator() we defined above).   Instead, we just use the formula for the minimum MSE to see what the mean square error is for each of the three possible choices of data X (i.e. for X=quiz scores, X=exam 1 scores, or X=exam 2 scores).  To get a better idea of what the MSE means, we also calculate the square root of the MSE  (i.e. the standard deviation of error) for each of the three linear estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean square error and root mean square error for predicting\n",
    "# score on final exam by using any one of the first three scores\n",
    "# Think of Y as representing final exam score, the fourth coordinate of the data vector x\n",
    "EY=mean_x[3]\n",
    "CorYX=correlation_matrix[3,0:3]  # correlation coefficients between Y and other three scores\n",
    "VarY=covariance_matrix[3,3]\n",
    "print (\"Mean square errors =\", VarY *(1-CorYX**2))\n",
    "print  (\"sqare roots of MSEs\", np.sqrt(VarY*(1-CorYX**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code shows that, of the three possibilities, the score on exam 2 is the best single predictor of the final exam score, and the standard deviation of the error for that estimator is about 24.\n",
    "\n",
    "Let's see how well the final exam score can be predicted by *combining* all three of the earlier scores, instead of just using one of them.  This is called *multiple linear regression.*  It turns out that the expressions above for best linear estimator extend to the case $X$ is vector valued.  ($Y$ can also be vector valued, but in that case, the estimation amounts to estimating each coordinate of $Y$ separately, anyway.)  If a random variable $Y$ is to be estimated from a\n",
    "random vector $X,$ the minimum mean square error linear estimator is given by\n",
    "$$ \\widehat{E}[Y | X] = E[Y] + \\mbox{Cov}(Y,X) \\mbox{Cov}(X,X)^{-1} (X-E[X]) $$\n",
    "and the mean square error is given by:\n",
    "$$E[(Y-\\widehat{E}[Y | X])^2] = \\mbox{Var}(Y) -  \\mbox{Cov}(Y,X) \\mbox{Cov}(X,X)^{-1} \\mbox{Cov}{(X,Y)}.$$\n",
    "Here, $\\mbox{Cov}{(X,Y)}$ is a matrix with $i,j^{th}$ entry equal to $\\mbox{Cov}(X_i,Y_j).$  Note that\n",
    "$( \\mbox{Cov}{(X,Y)})^T =  \\mbox{Cov}{(Y,X)}.$   \n",
    "\n",
    "To apply these equations to the numerical data, we think of the vector $X$ as the scores on the first three exams, and $Y$ as the score on the final exam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EX=mean_x[0:3]\n",
    "EY=mean_x[3]\n",
    "CovXX=covariance_matrix[0:3,0:3]\n",
    "CovXY=covariance_matrix[0:3,3]\n",
    "CovYX=covariance_matrix[3,0:3]\n",
    "print (CovYX)\n",
    "# assert all(CovXY==CovYX.T), \"CovXY is not the transpose of CovYX\"\n",
    "VarY=covariance_matrix[3,3]\n",
    "\n",
    "print (\"weight vector=\", np.dot(CovYX,np.linalg.inv(CovXX)))\n",
    "print (\"the constant=\", EY- np.dot( np.dot(CovYX,np.linalg.inv(CovXX)),EX))\n",
    "\n",
    "MSE= VarY -np.dot(CovYX,np.dot(np.linalg.inv(CovXX),CovXY))\n",
    "print (\"MSE for multiple regression\", MSE, \"square root of MSE\",np.sqrt(MSE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the best linear estimator of the final exam score given the first three scores is\n",
    "$$\\widehat{E}[\\mbox{final exam }|\\mbox{ other scores}]= (0.35)(\\mbox{quiz sum})+(0.67)(\\mbox{exam_1})+(0.85)(\\mbox{exam_2}) - 23.2$$ and the sqare root of the MSE (when applied to the data) is about 20.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<SPAN style=\"BACKGROUND-COLOR: #C0C0C0\">Problem 2:</SPAN>** Go through a similar process as above for predicting the sale price, Y, of housing in Los Angeles based on other factors such as number of bedrooms, square feet, etc.    Specifically, load the data from the file HousingPrices.csv, which has the data from the paper S.C. Narula and J.F. Wellington \"Prediction, Linear Regression, and Minimum Sum of Relative Error,\"  *Technometrics*, vol. 19, no. 2, May 1977, pp. 185-190.  Then:\n",
    "<ol>\n",
    "<li> Determine the minimum mean square error (MMSE) for estimation of $Y$ based on any one of the other numbers.  That is, for the MMSE for linear estimation of the sale price by the number of bathrooms, and the MMSE for estimation of the sale price by the inside square footage, etc.   Which of the other indicators gives the most accurate prediction of the sale price?\n",
    "<li> Find the MMSE linear estimator for estimation of $Y$ based on using all the other numbers combined. That is, find the weight vector and constant of the estimator.   Also, find the resulting value of the MSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__: (Your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<SPAN style=\"BACKGROUND-COLOR: #C0C0C0\">End of Problem 2</SPAN>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "## Academic Integrity Statement ##\n",
    "\n",
    "By submitting the lab with this statement, you declare you have written up the lab entirely by yourself, including both code and markdown cells. You also agree that you should not share your code with anyone else. Any violation of the academic integrity requirement may cause an academic integrity report to be filed that could go into your student record. See <a href=\"https://provost.illinois.edu/policies/policies/academic-integrity/students-quick-reference-guide-to-academic-integrity/\">Students' Quick Reference Guide to Academic Integrity</a> for more information. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
